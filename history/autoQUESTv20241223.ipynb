{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-26 17:39:08,135 [INFO] Logging is configured correctly.\n",
      "2024-12-26 17:39:08,136 [ERROR] This is a test error message.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging once at the beginning\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Test logging configuration\n",
    "logging.info(\"Logging is configured correctly.\")\n",
    "logging.error(\"This is a test error message.\")\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# PostgreSQL Database Connection Parameters\n",
    "DATABASE = {\n",
    "    'database': os.getenv('DB_NAME'),\n",
    "    'user': os.getenv('DB_USER'),\n",
    "    'password': os.getenv('DB_PASSWORD'),\n",
    "    'host': os.getenv('DB_HOST'),\n",
    "    'port': os.getenv('DB_PORT')\n",
    "}\n",
    "SCHEMA = 'maude'\n",
    "\n",
    "# Configure Google Generative AI API\n",
    "genai.configure(api_key=os.getenv('GENAI_API_KEY'))  # Store your API key securely\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "\n",
    "def generate_response(prompt):\n",
    "    \"\"\"\n",
    "    Generate a response using Google Generative AI based on the provided prompt.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calling Google Generative AI API: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_prompt_file(file_path):\n",
    "    \"\"\"\n",
    "    Read the content of the prompt file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def connect_database():\n",
    "    \"\"\"\n",
    "    Establish a connection to the PostgreSQL database.\n",
    "    \n",
    "    Returns:\n",
    "        psycopg2.connection: The active database connection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DATABASE)\n",
    "        logging.info(\"Successfully connected to the database.\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Database connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_table_structure(cursor, table_name):\n",
    "    \"\"\"\n",
    "    Retrieve the structure of a specified table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            column_name, \n",
    "            data_type, \n",
    "            character_maximum_length, \n",
    "            is_nullable\n",
    "        FROM \n",
    "            information_schema.columns\n",
    "        WHERE \n",
    "            table_schema = '{SCHEMA}' \n",
    "            AND table_name = '{table_name}';\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "        columns = cursor.fetchall()\n",
    "        structure = []\n",
    "        for col in columns:\n",
    "            structure.append({\n",
    "                'column_name': col[0],\n",
    "                'data_type': col[1],\n",
    "                'character_max_length': col[2],\n",
    "                'is_nullable': col[3]\n",
    "            })\n",
    "        return structure\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving table structure ({table_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def get_sample_data(cursor, table_name, limit=2):\n",
    "    \"\"\"\n",
    "    Retrieve sample data from a specified table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f'SELECT * FROM {SCHEMA}.\"{table_name}\" LIMIT {limit};'\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "        # Retrieve column names\n",
    "        col_names = [desc[0] for desc in cursor.description]\n",
    "        sample_data = [dict(zip(col_names, row)) for row in rows]\n",
    "        return sample_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving sample data ({table_name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_sql(sql_query):\n",
    "    \"\"\"\n",
    "    清除 SQL 查询中的注释和多余空白。\n",
    "    \"\"\"\n",
    "    # 去除单行注释\n",
    "    sql_query = re.sub(r'--.*', '', sql_query)\n",
    "    # 去除多行注释\n",
    "    sql_query = re.sub(r'/\\*.*?\\*/', '', sql_query, flags=re.S)\n",
    "    # 去除多余空格\n",
    "    return sql_query.strip()\n",
    "\n",
    "def execute_sql(conn, sql_query):\n",
    "    \"\"\"\n",
    "    Execute an SQL query and handle results, including operations without result sets.\n",
    "    Uses a context manager to ensure the cursor is properly closed after execution.\n",
    "    \n",
    "    Args:\n",
    "        conn (psycopg2.connection): The active database connection.\n",
    "        sql_query (str): The SQL query to be executed.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (data, error) where data is the result set for SELECT queries or None,\n",
    "               and error is the error message if an exception occurred, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            # 清理 SQL 查询\n",
    "            cleaned_query = clean_sql(sql_query)\n",
    "\n",
    "            # 如果清理后 SQL 为空，则不执行\n",
    "            if not cleaned_query:\n",
    "                return None, \"Empty or commented query.\"\n",
    "\n",
    "            # 设置 search_path\n",
    "            cursor.execute('SET search_path TO maude;')\n",
    "\n",
    "            # 判断是否为 CREATE VIEW 操作\n",
    "            if cleaned_query.lower().startswith(\"create view\"):\n",
    "                cursor.execute(cleaned_query)\n",
    "                conn.commit()  # 提交事务\n",
    "                return None, None  # CREATE VIEW 操作成功，但没有返回数据\n",
    "\n",
    "            # 判断是否为查询语句（返回结果集）\n",
    "            elif cleaned_query.lower().startswith((\"select\", \"with\", \"show\", \"describe\")):\n",
    "                cursor.execute(cleaned_query)\n",
    "                rows = cursor.fetchall()\n",
    "                col_names = [desc[0] for desc in cursor.description]\n",
    "                data = [dict(zip(col_names, row)) for row in rows]\n",
    "                return data, None\n",
    "\n",
    "            # 对于非查询操作（如 INSERT、UPDATE、DELETE 等）\n",
    "            else:\n",
    "                cursor.execute(cleaned_query)\n",
    "                conn.commit()  # 提交事务\n",
    "                return None, None  # 非查询操作成功，但没有返回数据\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"SQL Execution Error: {e}\")\n",
    "        try:\n",
    "            conn.rollback()  # 回滚事务以重置连接状态\n",
    "            logging.info(\"Transaction has been rolled back.\")\n",
    "        except Exception as rollback_error:\n",
    "            logging.error(f\"Failed to rollback transaction: {rollback_error}\")\n",
    "        return None, str(e)\n",
    "\n",
    "def extract_table_names(prompt_content):\n",
    "    \"\"\"\n",
    "    Extract real table names from the prompt content.\n",
    "    Assumes that table names are mentioned in the format `merged table xxx` and replaces them with actual table names.\n",
    "    For demonstration, uses a predefined mapping.\n",
    "    \"\"\"\n",
    "    # Define a mapping from merged table names to actual table names\n",
    "    # This should be updated based on actual mappings\n",
    "    merged_to_real = {\n",
    "        \"Merged_Table_1\": \"mdrfoi\",\n",
    "        \"Merged_Table_2\": \"patientproblemcode\",\n",
    "        \"Merged_Table_3\": \"some_other_table\",  # Replace with actual table names\n",
    "        # Add all necessary mappings here\n",
    "    }\n",
    "\n",
    "    # Extract merged table names using regex\n",
    "    merged_tables = re.findall(r'Merged_Table_\\d+', prompt_content)\n",
    "\n",
    "    # Replace merged table names with real table names\n",
    "    involved_tables = []\n",
    "    for mt in merged_tables:\n",
    "        real_table = merged_to_real.get(mt, None)\n",
    "        if real_table:\n",
    "            involved_tables.append(real_table)\n",
    "        else:\n",
    "            logging.warning(f\"No real table mapping found for {mt}. Please update the mapping.\")\n",
    "\n",
    "    # If no merged tables found, use default involved tables\n",
    "    if not involved_tables:\n",
    "        involved_tables = [\"mdrfoi\", \"ASR_2019\", \"DEVICE\", \"deviceproblemcodes\", \"patient\", \"patientproblemcode\", \"patientproblemdata\", \"foiclass\",\"foitext\", \"DISCLAIM\"]\n",
    "\n",
    "    return involved_tables\n",
    "\n",
    "def analyze_data(research_question, data):\n",
    "    \"\"\"\n",
    "    Analyze the data to validate the research question.\n",
    "    This function can be expanded based on specific analysis requirements.\n",
    "    For demonstration, it sends the data and research question to Generative AI for analysis.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        logging.warning(\"No data available for analysis.\")\n",
    "        return\n",
    "\n",
    "    # Convert data to JSON string\n",
    "    data_json = json.dumps(data, ensure_ascii=False)\n",
    "\n",
    "    # Create analysis prompt\n",
    "    analysis_prompt = (\n",
    "        f\"Based on the following research question and data, list the research question firstly, make interpretation and insights on returned data (use Tables to present the insights if possible, **DO NOT FAKE DATA**) secondly and then analyze the validity and feasibility of the research question.\\n\\n\"\n",
    "        f\"Research Question: {research_question}\\n\\n\"\n",
    "        f\"Data: {data_json}\\n\\n\"\n",
    "        f\"Provide a detailed analysis report:\"\n",
    "    )\n",
    "\n",
    "    # Get analysis report from Generative AI\n",
    "    analysis_report = generate_response(analysis_prompt)\n",
    "    if analysis_report:\n",
    "        logging.info(f\"Analysis Report:\\n{analysis_report}\")\n",
    "        # Write the analysis report to a file\n",
    "        with open(\"finalreport.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"# Analysis Report\\n\\n{analysis_report}\\n\")\n",
    "        logging.info(\"Analysis report successfully written to finalreport.md.\")\n",
    "    else:\n",
    "        logging.error(\"Failed to generate analysis report.\")\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "def parse_prompt_txt(table_info: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse prompt.txt file to extract pseudo table to real table and field mappings.\n",
    "    \n",
    "    Args:\n",
    "        table_info (str): Content from prompt.txt\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Mapping of pseudo tables to real tables and fields\n",
    "    \"\"\"\n",
    "    pseudo_tables = {}\n",
    "    table_pattern = re.compile(\n",
    "        r\"Table\\s+'(?P<pseudo_table>[^']+)'\\s+\\(merged from:\\s+([^)]*)\\):\\s+Fields:\\s+(?P<fields>[^.]+)\\.\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    for match in table_pattern.finditer(table_info):\n",
    "        pseudo_table = match.group('pseudo_table').strip()\n",
    "        real_tables_str = match.group(2).strip()\n",
    "        fields_str = match.group('fields').strip()\n",
    "        \n",
    "        real_tables = [tbl.strip() for tbl in real_tables_str.split(',')]\n",
    "        fields = []\n",
    "        field_pattern = re.compile(r\"([A-Za-z0-9_]+)\\s+\\([^()]+\\)\")\n",
    "        for field_match in field_pattern.finditer(fields_str):\n",
    "            field_name = field_match.group(1).strip()\n",
    "            fields.append(field_name)\n",
    "            \n",
    "        pseudo_tables[pseudo_table] = {\n",
    "            'real_tables': real_tables,\n",
    "            'fields': fields\n",
    "        }\n",
    "    \n",
    "    return pseudo_tables\n",
    "\n",
    "def get_real_table_name(pseudo_tables: Dict, pseudo_table: str) -> str:\n",
    "    \"\"\"\n",
    "    Get a real table name from the pseudo table mapping.\n",
    "    \"\"\"\n",
    "    if pseudo_table not in pseudo_tables:\n",
    "        logging.error(f\"Pseudo table '{pseudo_table}' not found in mapping.\")\n",
    "        return None\n",
    "        \n",
    "    real_tables = pseudo_tables[pseudo_table]['real_tables']\n",
    "    if not real_tables:\n",
    "        logging.error(f\"Pseudo table '{pseudo_table}' has no corresponding real tables.\")\n",
    "        return None\n",
    "        \n",
    "    chosen_table = random.choice(real_tables)\n",
    "    return chosen_table\n",
    "\n",
    "def process_execution_steps_and_tables(execution_steps: str, table_info: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Process execution steps and extract involved tables.\n",
    "    \n",
    "    Args:\n",
    "        execution_steps (str): The execution steps text containing SQL queries\n",
    "        table_info (str): Content from prompt.txt\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: (Updated execution steps, List of involved tables)\n",
    "    \"\"\"\n",
    "    # Parse table mapping information\n",
    "    pseudo_tables_mapping = parse_prompt_txt(table_info)\n",
    "    \n",
    "    # Regular expressions to find table references\n",
    "    table_patterns = [\n",
    "        r\"(?:FROM|JOIN)\\s+(?:Merged_Table_\\d+)\",\n",
    "        r\"<Result from (?:Join )?Step \\d+>\",\n",
    "        r\"Merged_Table_\\d+\",\n",
    "    ]\n",
    "    \n",
    "    # Find all pseudo table references\n",
    "    pseudo_tables = set()\n",
    "    for pattern in table_patterns:\n",
    "        matches = re.finditer(pattern, execution_steps, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            table_ref = re.search(r'(Merged_Table_\\d+)', match.group(0))\n",
    "            if table_ref:\n",
    "                pseudo_tables.add(table_ref.group(1))\n",
    "    \n",
    "    # Map pseudo tables to real tables and update execution steps\n",
    "    involved_tables = []\n",
    "    updated_steps = execution_steps\n",
    "    \n",
    "    for pseudo_table in pseudo_tables:\n",
    "        real_table = get_real_table_name(pseudo_tables_mapping, pseudo_table)\n",
    "        if real_table:\n",
    "            # Update execution steps\n",
    "            updated_steps = re.sub(\n",
    "                rf'\\b{pseudo_table}\\b',\n",
    "                real_table,\n",
    "                updated_steps\n",
    "            )\n",
    "            # Add to involved tables if not already present\n",
    "            if real_table not in involved_tables:\n",
    "                involved_tables.append(real_table)\n",
    "    \n",
    "    # Handle intermediate result references\n",
    "    step_pattern = r'<Result from (?:Join )?Step \\d+>'\n",
    "    step_matches = re.finditer(step_pattern, updated_steps)\n",
    "    for match in step_matches:\n",
    "        step_num = re.search(r'\\d+', match.group(0)).group(0)\n",
    "        updated_steps = updated_steps.replace(\n",
    "            match.group(0),\n",
    "            f'step_{step_num}_result'\n",
    "        )\n",
    "    \n",
    "    # Sort involved tables for consistency\n",
    "    involved_tables = sorted(involved_tables)\n",
    "    \n",
    "    # Add standard tables that are always involved\n",
    "    standard_tables = []\n",
    "    \n",
    "    # Merge and deduplicate while maintaining order\n",
    "    final_tables = []\n",
    "    for table in standard_tables:\n",
    "        if table not in final_tables:\n",
    "            final_tables.append(table)\n",
    "    for table in involved_tables:\n",
    "        if table not in final_tables:\n",
    "            final_tables.append(table)\n",
    "    \n",
    "    return updated_steps, final_tables\n",
    "\n",
    "def generate_dqc_plan(execution_steps, table_info, add_content):\n",
    "    \"\"\"\n",
    "    Generate a Data Quality Check (DQC) plan using GenAI based on execution steps.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Please create a detailed Data Quality Check (DQC) plan based on the following execution steps. \"\n",
    "        \"Focus only on the fields involved in the execution steps, not all fields of the tables.\\n\\n\"\n",
    "        f\"Confirmed MAUDE Database Table Structures and Sample Data:\\n{json.dumps(table_info, ensure_ascii=False, indent=2)}\\n\\n\"\n",
    "        f\"Additional Table Information:\\n{add_content}\\n\\n\"\n",
    "        f\"Optimized Execution Steps:\\n{execution_steps}\\n\\n\"\n",
    "        \"The DQC plan should include the following aspects:\\n\"\n",
    "        \"1. Field Existence Checks\\n\"\n",
    "        \"2. Field Type Consistency Checks\\n\"\n",
    "        \"3. Logical Relationships Between Fields\\n\"\n",
    "        \"4. Data Cleaning and Transformation Validation\\n\"\n",
    "        \"5. Potential Data Quality Issues and Recommendations\\n\\n\"\n",
    "        \"Please outline the strategies and corresponding SQL queries needed to perform these checks.\\n \"\n",
    "        \"Provide the SQL queries in code blocks within ```sql``` code fences.\\n\"\n",
    "        \"Ensure that table names are formatted as ** \\\"maude\\\".\\\"tablename\\\" ** and use the correct table and column names as per the Confirmed Table Structures and Data Samples.\\n\"\n",
    "        \"Do NOT generate any SQL queries that may modify data, such as UPDATE, DELETE, INSERT, CREATE TABLE...\\n\\n\"\n",
    "        \"Please ensure that each SQL query does not return too many hits by including a LIMIT 10 clause, distinct function or other techniques.\\n\\n\"\n",
    "        \"Provide the SQL queries in the following format:\\n\\n\"\n",
    "        \"```sql\\nSELECT * FROM \\\"maude\\\".\\\"tablename\\\" LIMIT 10;\\n```\\n\"\n",
    "    )\n",
    "    \n",
    "    dqc_plan = generate_response(prompt)\n",
    "    if not dqc_plan:\n",
    "        logging.error(\"Failed to generate DQC plan.\")\n",
    "    return dqc_plan\n",
    "\n",
    "def extract_sql_queries(dqc_plan):\n",
    "    \"\"\"\n",
    "    Extract SQL queries from the DQC plan.\n",
    "    \"\"\"\n",
    "    pattern = r'```sql\\n(.*?)```'\n",
    "    matches = re.findall(pattern, dqc_plan, re.DOTALL)\n",
    "    sql_queries = [match.strip() for match in matches]\n",
    "    return sql_queries\n",
    "\n",
    "def execute_query_list(sql_queries, table_info):\n",
    "    \"\"\"\n",
    "    Execute the DQC SQL queries and collect results.\n",
    "    Manages database connection and handles query execution with error correction.\n",
    "    \n",
    "    Args:\n",
    "        sql_queries (list): List of SQL queries for data quality checks.\n",
    "        table_info (dict): Confirmed table structures and sample data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with SQL queries as keys and their execution results or errors as values.\n",
    "    \"\"\"\n",
    "    # Connect to the database\n",
    "    conn = connect_database()\n",
    "    if not conn:\n",
    "        logging.error(\"Database connection failed.\")\n",
    "        return {}\n",
    "    \n",
    "    # Initialize a dictionary to store results and a list to accumulate all data\n",
    "    dqc_results = {}\n",
    "    dataall = []\n",
    "    max_retries = 5  # Increased retries to handle persistent issues\n",
    "    \n",
    "    try:\n",
    "        for idx, sql_query in enumerate(sql_queries, start=1):\n",
    "            logging.info(f\"Executing DQC SQL Query {idx}/{len(sql_queries)}:\\n{sql_query}\\n\")\n",
    "            attempt = 0\n",
    "            while attempt < max_retries:\n",
    "                data, error = execute_sql(conn, sql_query)\n",
    "                if error:\n",
    "                    logging.error(f\"SQL Execution Error on DQC Query {idx}: {error}\\n\")\n",
    "    \n",
    "                    if \"current transaction is aborted\" in error.lower():\n",
    "                        try:\n",
    "                            conn.rollback()\n",
    "                            logging.info(\"Rolled back the aborted transaction.\")\n",
    "                        except Exception as rollback_error:\n",
    "                            logging.error(f\"Failed to rollback transaction: {rollback_error}\")\n",
    "                            break\n",
    "                        attempt += 1\n",
    "                        continue\n",
    "    \n",
    "                    # Prepare the correction prompt for Generative AI with code fences\n",
    "                    correction_prompt = (\n",
    "                        f\"The following SQL query resulted in an error. Please correct it based on the error message and the table information.\\n\\n\"\n",
    "                        f\"Original SQL Query:\\n{sql_query}\\n\\n\"\n",
    "                        f\"Error Message: {error}\\n\\n\"\n",
    "                        f\"Table Information: {json.dumps(table_info, ensure_ascii=False, indent=2)}\\n\\n\"\n",
    "                        f\"Provide the corrected SQL query enclosed within ```sql``` code fences.\\n\"\n",
    "                        f\"Do NOT include any additional commentary or text.\"\n",
    "                    )\n",
    "                    time.sleep(3)  # Delay to avoid frequent requests\n",
    "                    corrected_sql_full = generate_response(correction_prompt)\n",
    "    \n",
    "                    if not corrected_sql_full:\n",
    "                        logging.warning(\"Failed to correct SQL query. Skipping to the next query.\\n\")\n",
    "                        dqc_results[f\"DQC Query {idx}\"] = {\"error\": error}\n",
    "                        break\n",
    "    \n",
    "                    logging.info(f\"Corrected DQC SQL Query {idx}:\\n\\n{corrected_sql_full}\\n\")\n",
    "    \n",
    "                    # Extract the corrected SQL query from ```sql``` code fences\n",
    "                    pattern = r'```sql\\s*\\n(.*?)```'\n",
    "                    matches = re.findall(pattern, corrected_sql_full, re.DOTALL | re.IGNORECASE)\n",
    "                    if matches:\n",
    "                        # Assume the first match is the corrected query\n",
    "                        corrected_query = matches[0].strip()\n",
    "                        logging.info(f\"Updating DQC Query {idx} with corrected SQL.\")\n",
    "                        sql_query = corrected_query\n",
    "                    else:\n",
    "                        logging.warning(\"No SQL code block found in the corrected response. Skipping to the next query.\")\n",
    "                        dqc_results[f\"DQC Query {idx}\"] = {\"error\": error}\n",
    "                        break\n",
    "    \n",
    "                    attempt += 1\n",
    "                    time.sleep(1)  # Brief pause before retrying\n",
    "                else:\n",
    "                    logging.info(f\"DQC SQL Query {idx} executed successfully.\\n\")\n",
    "                    if data:\n",
    "                        dqc_results[f\"DQC Query {idx}\"] = {\"data\": data}\n",
    "                        dataall.extend(data)\n",
    "                        logging.info(f\"Retrieved {len(data)} records from DQC Query {idx}.\\n\")\n",
    "                    else:\n",
    "                        dqc_results[f\"DQC Query {idx}\"] = {\"data\": None}\n",
    "                        logging.info(f\"No data returned from DQC Query {idx}.\\n\")\n",
    "                    break\n",
    "    \n",
    "            # After retries, check if the last attempt resulted in an error\n",
    "            if error and attempt == max_retries:\n",
    "                logging.error(f\"Reached maximum retry attempts for DQC Query {idx}. Unable to execute this query.\\n\")\n",
    "                dqc_results[f\"DQC Query {idx}\"] = {\"error\": error}\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the database connection is closed properly\n",
    "        try:\n",
    "            conn.close()\n",
    "            logging.info(\"Database connection closed.\")\n",
    "        except Exception as close_error:\n",
    "            logging.error(f\"Error closing database connection: {close_error}\")\n",
    "    \n",
    "    # Correctly calculate the total records\n",
    "    total_records = sum(len(v[\"data\"]) for v in dqc_results.values() if v.get(\"data\"))\n",
    "    logging.info(f\"Total records retrieved from all queries: {total_records}\")\n",
    "    \n",
    "    return dqc_results\n",
    "\n",
    "def generate_dqc_report(dqc_plan, dqc_results):\n",
    "    \"\"\"\n",
    "    Generate a Data Quality Control report using Generative AI based on the DQC plan and results.\n",
    "    \"\"\"\n",
    "    # Convert DQC results to a readable format\n",
    "    dqc_results_str = json.dumps(dqc_results, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    prompt = (\n",
    "        \"Please generate a detailed Data Quality Control (DQC) report based on the following plan and execution results. \"\n",
    "        \"The report should be in Markdown format and include the following sections:\\n\"\n",
    "        \"1. Introduction\\n\"\n",
    "        \"2. Data Quality Check Plan\\n\"\n",
    "        \"3. Execution Results\\n\"\n",
    "        \"4. Analysis and Recommendations\\n\\n\"\n",
    "        \"### Data Quality Check Plan:\\n\"\n",
    "        f\"{dqc_plan}\\n\\n\"\n",
    "        \"### Execution Results:\\n\"\n",
    "        f\"```json\\n{dqc_results_str}\\n```\\n\\n\"\n",
    "        \"### Analysis and Recommendations:\\n\"\n",
    "        \"Based on the execution results, analyze the data quality issues identified and provide recommendations for improvement.\"\n",
    "    )\n",
    "    \n",
    "    dqc_report = generate_response(prompt)\n",
    "    if not dqc_report:\n",
    "        logging.error(\"Failed to generate DQC report.\")\n",
    "    return dqc_report\n",
    "\n",
    "def perform_data_quality_control(execution_steps_new, table_info, add_content):\n",
    "    \"\"\"\n",
    "    Perform the entire Data Quality Control process:\n",
    "    1. Generate DQC plan\n",
    "    2. Extract and execute SQL queries\n",
    "    3. Generate DQC report\n",
    "    \"\"\"\n",
    "    # 1. Generate DQC Plan\n",
    "    dqc_plan = generate_dqc_plan(execution_steps_new, table_info, add_content)\n",
    "    if not dqc_plan:\n",
    "        logging.error(\"Failed to generate Data Quality Check plan.\")\n",
    "        return\n",
    "    \n",
    "    logging.info(\"Data Quality Check Plan:\\n\")\n",
    "    logging.info(dqc_plan)\n",
    "    \n",
    "    # 2. Extract SQL Queries from DQC Plan\n",
    "    sql_queries = extract_sql_queries(dqc_plan)\n",
    "    if not sql_queries:\n",
    "        logging.error(\"No SQL queries found in the DQC plan.\")\n",
    "        return\n",
    "    \n",
    "    logging.info(\"Extracted DQC SQL Queries:\\n\")\n",
    "    for idx, query in enumerate(sql_queries, start=1):\n",
    "        logging.info(f\"--- DQC SQL Query {idx} ---\")\n",
    "        logging.info(query)\n",
    "        logging.info(\"\\n\")\n",
    "    \n",
    "    # 3. Execute SQL Queries and Gather Results\n",
    "    dqc_results = execute_query_list(sql_queries, table_info)\n",
    "    \n",
    "    # 4. Generate DQC Report\n",
    "    dqc_report = generate_dqc_report(dqc_plan, dqc_results)\n",
    "    if dqc_report:\n",
    "        logging.info(\"\\nData Quality Control Report:\\n\")\n",
    "        logging.info(dqc_report)\n",
    "        \n",
    "        # Save the report to a Markdown file\n",
    "        try:\n",
    "            with open(\"data_quality_report.md\", \"w\", encoding=\"utf-8\") as report_file:\n",
    "                report_file.write(dqc_report)\n",
    "            logging.info(\"Data Quality Control Report has been successfully saved to data_quality_report.md.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to write DQC report to file: {e}\")\n",
    "    else:\n",
    "        logging.error(\"Failed to generate Data Quality Control report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Read table information from prompt.txt\n",
    "prompt_file = 'prompt.txt'\n",
    "prompt_content = read_prompt_file(prompt_file)\n",
    "if not prompt_content:\n",
    "    logging.error(\"Failed to read prompt.txt file.\")\n",
    "    exit(1)  # Exit if prompt.txt cannot be read\n",
    "\n",
    "# 1.2 Read additional table information from metadata.txt\n",
    "add_file = 'metadata.txt'\n",
    "add_content = read_prompt_file(add_file)\n",
    "if not add_content:\n",
    "    logging.error(\"Failed to read metadata.txt file.\")\n",
    "    exit(1)  # Exit if metadata.txt cannot be read\n",
    "\n",
    "# 2. Generate a research question\n",
    "research_prompt = (\n",
    "    f\"Based on the following MAUDE database table information, propose a meaningful research question and strategy.\\n\\n\\n\\n\"\n",
    "    f\"\\n\\n{prompt_content}\\n\\n\"\n",
    "    f\"Additional Table Information: \\n{add_content}\\n\\n\"\n",
    ")\n",
    "research_question = generate_response(research_prompt)\n",
    "if not research_question:\n",
    "    logging.error(\"Failed to generate a research question.\")\n",
    "    exit(1)  # Exit if research question cannot be generated\n",
    "logging.info(f\"Proposed Research Question:\\n\\n{research_question}\\n\")\n",
    "\n",
    "# 3. Plan execution steps based on the research question\n",
    "planning_prompt = (\n",
    "    f\"Based on the following research question, outline specific execution steps, including which tables and fields need to be queried.\\n\\n\\n\\n\"\n",
    "    f\"Research Question: \\n{research_question}\"\n",
    ")\n",
    "execution_steps = generate_response(planning_prompt)\n",
    "if not execution_steps:\n",
    "    logging.error(\"Failed to plan execution steps.\")\n",
    "    exit(1)  # Exit if execution steps cannot be planned\n",
    "logging.info(f\"Planned Execution Steps:\\n\\n{execution_steps}\\n\")\n",
    "# 4. Identify involved tables by extracting from execution steps\n",
    "updated_steps, involved_tables = process_execution_steps_and_tables(execution_steps, prompt_content)\n",
    "\n",
    "logging.info(\"Updated execution steps:\")\n",
    "logging.info(updated_steps)\n",
    "logging.info(\"\\nInvolved tables:\")\n",
    "logging.info(involved_tables)\n",
    "\n",
    "# 5. Acquire Involved Table Info, Check table structures and sample data\n",
    "# Connect to the database\n",
    "conn = connect_database()\n",
    "if not conn:\n",
    "    logging.error(\"Database connection failed.\")\n",
    "    exit(1)  # Exit the script if the connection fails\n",
    "\n",
    "table_info = {}\n",
    "try:\n",
    "    with conn.cursor() as cursor:\n",
    "        for table in involved_tables:\n",
    "            # Check if table exists\n",
    "            structure = get_table_structure(cursor, table)\n",
    "            if structure is None or not structure:\n",
    "                logging.warning(f\"Table '{table}' does not exist or has no columns. Skipping.\")\n",
    "                continue  # Skip to the next table if structure is invalid\n",
    "            \n",
    "            # Retrieve sample data\n",
    "            samples = get_sample_data(cursor, table)\n",
    "            if samples is None:\n",
    "                logging.warning(f\"Unable to retrieve sample data for table: {table}\")\n",
    "                continue  # Skip to the next table if samples cannot be retrieved\n",
    "            \n",
    "            # Populate table_info dictionary\n",
    "            table_info[table] = {\n",
    "                'structure': structure,\n",
    "                'samples': samples\n",
    "            }\n",
    "            \n",
    "            # Optional: Log table structure and sample data for verification\n",
    "            # logging.debug(f\"Table: {table}\")\n",
    "            # logging.debug(f\"Structure: {json.dumps(structure, ensure_ascii=False, indent=2)}\")\n",
    "            # logging.debug(f\"Sample Data: {json.dumps(samples, ensure_ascii=False, indent=2, default=serialize)}\\n\")\n",
    "    \n",
    "    if not table_info:\n",
    "        logging.error(\"No valid tables found for analysis. Exiting.\")\n",
    "        exit(1)  # Exit the script if no valid tables are found\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"An error occurred while accessing the database: {e}\")\n",
    "    exit(1)  # Exit the script on database errors\n",
    "\n",
    "finally:\n",
    "    # Ensure the database connection is closed properly\n",
    "    try:\n",
    "        conn.close()\n",
    "        logging.info(\"Database connection closed.\")\n",
    "    except Exception as close_error:\n",
    "        logging.error(f\"Error closing database connection: {close_error}\")\n",
    "\n",
    "logging.info(f\"Table Information: {json.dumps(table_info, ensure_ascii=False, indent=2)}\")\n",
    "\n",
    "# 7. Polish Execution Steps w/ Confirmed Table Info and Add Info\n",
    "planning_prompt_new = (\n",
    "    f\"Optimize Execution Steps, Prevent table, field name and value format from errors according to Table Structures and Data Samples: \\n{json.dumps(table_info, ensure_ascii=False, indent=2)}\\n\\n\"\n",
    "    f\"Additional Table Information: \\n{add_content}\\n\\n\"\n",
    "    f\"Based on the above Confirmed information of MAUDE Database Structures and Data Samples and the following Execution Steps, polish the specific execution steps, especially on correcting the name and logic of tables and columns that need to be queried.\\n\\n\\n\\n\"\n",
    "    f\"Current Execution Steps: \\n{updated_steps}\\n\\n\"\n",
    ")\n",
    "# print(planning_prompt_new)\n",
    "execution_steps_new = generate_response(planning_prompt_new)\n",
    "if not execution_steps_new:\n",
    "    logging.error(\"Failed to polish execution steps.\")\n",
    "    exit(1)\n",
    "logging.info(f\"Optimized Execution Steps:\\n\\n{execution_steps_new}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SQL queries with instructions\n",
    "sql_prompt = (\n",
    "    f\"Based on the following execution steps and confirmed table structures and data samples, generate SQL queries.\\n\\n\"\n",
    "    f\"Execution Steps: \\n{execution_steps_new}\\n\\n\"\n",
    "    f\"Confirmed Table Structures and Data Samples: \\n{json.dumps(table_info, ensure_ascii=False, indent=2)}\\n\\n\"\n",
    "    f\"Ensure that table names are formatted as **\\\"maude\\\".\\\"tablename\\\"** and use the correct table and column names as per the Confirmed Table Structures and Data Samples.\\n\\n\"\n",
    "    f\"Please ensure that each SQL query does not return more than 10 hits by including a LIMIT 10 clause.\\n\\n\"\n",
    "    f\"Each generated SQL statement should be enclosed within ```sql``` code fences, be self-contained and independent, meaning they should not rely on the execution of other SQL statements. If there are dependencies between queries, combine them into a single, cohesive SQL statement.\\n\\n\"\n",
    "    #f\"Use no more than 10 simple SQL queries to fulfill the execution.\\n\\n\"\n",
    "    f\"Do NOT generate any SQL queries that may modify data, such as UPDATE, DELETE, INSERT, CREATE TABLE...\\n\\n\"\n",
    "    f\"Provide the SQL queries in the following format:\\n\\n\"\n",
    "    f\"```sql\\nSELECT * FROM \\\"maude\\\".\\\"tablename\\\" LIMIT 10;\\n```\\n\"\n",
    ")\n",
    "\n",
    "# Generate SQL queries with instructions in JSON format\n",
    "sql_query_full = generate_response(sql_prompt)\n",
    "if not sql_query_full:\n",
    "    logging.error(\"Failed to generate SQL queries.\")\n",
    "    exit(1)  # Exit if SQL queries cannot be generated\n",
    "\n",
    "logging.info(f\"Generated SQL Queries JSON:\\n\\n{sql_query_full}\\n\")\n",
    "\n",
    "sql_queries = extract_sql_queries(sql_query_full)\n",
    "\n",
    "# Execute DQC Queries\n",
    "sql_results = execute_query_list(sql_queries, table_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Analyze data to validate the research question\n",
    "if sql_results:\n",
    "    # Construct the formatted string and assign it to a new variable\n",
    "    tmp = (\n",
    "        f\"Execution Steps: {execution_steps_new}\\n\\n\"\n",
    "        f\"SQL Queries: {sql_queries}\\n\\n\"\n",
    "        f\"SQL Execution Outcome:\\n{json.dumps(sql_results, ensure_ascii=False, indent=2)}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    analyze_data(research_question, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_data_quality_control(execution_steps_new, table_info, add_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# 假设您已在别处定义 generate_response 与 logging 配置\n",
    "# from your_openai_module import generate_response\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "def sanitize_code(code_str: str) -> str:\n",
    "    \"\"\"\n",
    "    去除可能导致语法错误的 Markdown 代码块标记和其他无效字符，\n",
    "    并去掉前后空白，保证可以被 Python 解释器正确执行。\n",
    "    \"\"\"\n",
    "    code_str = re.sub(r'```python\\s*', '', code_str, flags=re.IGNORECASE)\n",
    "    code_str = re.sub(r'```', '', code_str, flags=re.IGNORECASE)\n",
    "    code_str = code_str.strip()\n",
    "    # 若有 BOM 或不可见字符:\n",
    "    # code_str = code_str.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
    "    return code_str\n",
    "\n",
    "def execute_generated_code(analysis_code, sql_results):\n",
    "    \"\"\"\n",
    "    执行生成的Python分析代码。\n",
    "    \n",
    "    Args:\n",
    "        analysis_code (str): 生成的Python代码片段。\n",
    "        sql_results (dict): SQL 查询返回的数据结果。\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success: bool, output: str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        local_namespace = {}\n",
    "        exec_globals = globals().copy()\n",
    "        # 使生成的代码能直接访问 sql_results\n",
    "        exec_globals['sql_results'] = sql_results\n",
    "\n",
    "        from io import StringIO\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = StringIO()\n",
    "        try:\n",
    "            exec(analysis_code, exec_globals, local_namespace)\n",
    "            output = sys.stdout.getvalue()\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "        \n",
    "        return True, output\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def perform_data_analysis(execution_steps_new, sql_queries, sql_results, max_retries=3):\n",
    "    \"\"\"\n",
    "    自动生成、执行并调试分析代码：\n",
    "      1) 基于 execution_steps_new 和 sql_results \n",
    "      2) 要求只使用已有的 sql_results\n",
    "      3) 可视化结果用 plt.savefig('xxx.png') 方式保存\n",
    "      4) 最多可重试 max_retries 次\n",
    "    \"\"\"\n",
    "    # 限制 sql_results 的数据量，防止数据过大\n",
    "    # limited_sql_results = {k: v[:2] for k, v in sql_results.items() if isinstance(v, list)}\n",
    "    \n",
    "    analysis_prompt = (\n",
    "        \"请使用现有的 sql_results 变量(已在执行环境中)，将其转换为 pandas.DataFrame 进行分析。\"\n",
    "        \"不要在代码里赋值 sql_results。\"\n",
    "        \"根据以下执行步骤(仅做参考，不用回显)、以及 sql_results 的数据结构，\"\n",
    "        \"编写简洁的 Python 代码做一些基础分析和可视化:\"\n",
    "        \"  - 分析可以包含: 统计描述、groupby、简单图表等\"\n",
    "        \"  - 每个图表以 plt.savefig('xxx.png') 保存, 不要 plt.show()\"\n",
    "        \"  - 代码要尽量精简，可读性高，注释简洁\"\n",
    "        \"  - 不要包含任何三引号或 ``` 代码块标记\\n\\n\"\n",
    "        f\"执行步骤(参考): {execution_steps_new}\\n\\n\"\n",
    "        f\"示例数据结构(参考): {sql_queries}\\n\\n\"\n",
    "        f\"示例 sql_results(部分): {json.dumps(sql_results, ensure_ascii=False, indent=2)}\\n\\n\"\n",
    "        \"请生成纯 Python 代码，不要多余解释。\"\n",
    "    )\n",
    "\n",
    "    retry_count = 0\n",
    "    success = False\n",
    "    analysis_code = \"\"\n",
    "    original_code = \"\"\n",
    "    error_message = \"\"\n",
    "\n",
    "    while retry_count < max_retries and not success:\n",
    "        if retry_count == 0:\n",
    "            # 第一次生成\n",
    "            analysis_code = generate_response(analysis_prompt)\n",
    "            if not analysis_code:\n",
    "                logging.error(\"生成分析代码失败。\")\n",
    "                return\n",
    "            analysis_code = sanitize_code(analysis_code)\n",
    "        else:\n",
    "            # 调试修正\n",
    "            debug_prompt = (\n",
    "                f\"上次执行的代码报错：{error_message}\\n\"\n",
    "                f\"原始代码:\\n{original_code}\\n\\n\"\n",
    "                f\"请直接使用已有的 sql_results，不要在代码里赋值 sql_results。\"\n",
    "                f\"图表用 plt.savefig('xxx.png')。保持代码简洁，删除多余逻辑和说明。\\n\\n\"\n",
    "                f\"执行步骤(参考): {execution_steps_new}\\n\\n\"\n",
    "                f\"示例数据结构(参考): {sql_queries}\\n\\n\"\n",
    "                f\"示例 sql_results(部分): {json.dumps(sql_results, ensure_ascii=False, indent=2)}\\n\\n\"\n",
    "            )\n",
    "            analysis_code = generate_response(debug_prompt)\n",
    "            if not analysis_code:\n",
    "                logging.error(\"生成修正后的分析代码失败。\")\n",
    "                return\n",
    "            analysis_code = sanitize_code(analysis_code)\n",
    "\n",
    "        logging.info(f\"第 {retry_count + 1} 次生成/修正的代码:\\n{analysis_code}\\n\")\n",
    "        \n",
    "        success, output = execute_generated_code(analysis_code, sql_results)\n",
    "        if success:\n",
    "            with open(\"analysis_output.log\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(output)\n",
    "            logging.info(\"分析代码执行成功，输出已保存到 analysis_output.log。\")\n",
    "            break\n",
    "        else:\n",
    "            logging.error(f\"分析代码执行失败，第 {retry_count + 1} 次。错误信息:\\n{output}\")\n",
    "            original_code = analysis_code\n",
    "            error_message = output\n",
    "            retry_count += 1\n",
    "\n",
    "    if not success:\n",
    "        logging.error(f\"已尝试 {max_retries} 次，仍未成功执行分析代码。\")\n",
    "\n",
    "# 在主流程中调用:\n",
    "perform_data_analysis(execution_steps_new, sql_queries, sql_results, max_retries=3)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
